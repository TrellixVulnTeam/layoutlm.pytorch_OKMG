{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import arrow\n",
    "# import random\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# from datetime import datetime\n",
    "# from pandas.core.common import flatten\n",
    "# from collections import OrderedDict\n",
    "\n",
    "\n",
    "# import json\n",
    "# import chardet\n",
    "\n",
    "# import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, AutoModel\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from laylm.data import utils \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "class IDCardDataset(dataset.Dataset):\n",
    "    def __init__(self, root, tokenizer, labels=None, mode='train', \n",
    "                 test_size=0.2, max_seq_length=512):\n",
    "        self.root = Path(root)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.mode = mode\n",
    "        self.test_size = test_size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self._build_files()\n",
    "        \n",
    "    def _build_files(self):\n",
    "        names = self._get_names(\"*_json.json\")\n",
    "        data = self._get_filtered_files(names)\n",
    "        dframe, train, test = self._split_dataset(data)\n",
    "        self.data_frame = dframe\n",
    "        self.train_frame = train\n",
    "        self.test_frame = test\n",
    "        \n",
    "        if self.mode==\"train\":\n",
    "            self.frame = self.train_frame\n",
    "        else:\n",
    "            self.frame = self.test_frame\n",
    "        \n",
    "    def _split_dataset(self, data):\n",
    "        dframe = pd.DataFrame(data)\n",
    "        train, test = train_test_split(dframe, \n",
    "                                       test_size=self.test_size, \n",
    "                                       random_state=1261)\n",
    "        train = train.reset_index(drop=True)\n",
    "        test = test.reset_index(drop=True)\n",
    "        return dframe, train, test\n",
    "        \n",
    "    def _get_filtered_files(self, names):\n",
    "        data = {'name':[], 'image':[], 'mask':[],'anno':[]}\n",
    "        \n",
    "        jfiles = self._glob_filter(\"*_json.json\")\n",
    "        ifiles = self._glob_filter(\"*_image.jpg\")\n",
    "        mfiles = self._glob_filter(\"*_mask.jpg\")\n",
    "        \n",
    "        for name in names:\n",
    "            for (jfile, ifile, mfile)  in zip(jfiles, ifiles, mfiles):\n",
    "                \n",
    "                jfpn = jfile.name.split(\"_\")[0]\n",
    "                ifpn = ifile.name.split(\"_\")[0]\n",
    "                mfpn = mfile.name.split(\"_\")[0]\n",
    "                \n",
    "                if name == jfpn and name == ifpn and name == mfpn:\n",
    "                    data['name'].append(name)\n",
    "                    data['image'].append(ifile)\n",
    "                    data['mask'].append(mfile)\n",
    "                    data['anno'].append(jfile)\n",
    "                    \n",
    "        return data\n",
    "\n",
    "    def _get_names(self, path_pattern):\n",
    "        names = []\n",
    "        files = self._glob_filter(path_pattern)\n",
    "        for file in files:\n",
    "            names.append(file.name.split(\"_\")[0])\n",
    "        return names\n",
    "    \n",
    "    def _glob_filter(self, pattern):\n",
    "        return sorted(list(self.root.glob(pattern)))\n",
    "    \n",
    "    def _load_anno(self, path):\n",
    "        path = str(path)\n",
    "        with open(path) as f:\n",
    "            data_dict = json.load(f)\n",
    "        return data_dict\n",
    "    \n",
    "    def _load_image(self, path):\n",
    "        path = str(path)\n",
    "        img = cv.imread(path, cv.IMREAD_UNCHANGED)\n",
    "        return img\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frame)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        record = self.frame.iloc[idx]\n",
    "        anno = self._load_anno(record['anno'])\n",
    "        img = self._load_image(record['image'])\n",
    "        mask = self._load_image(record['mask'])\n",
    "        \n",
    "        anno_objects = utils.format_annotation_objects(\n",
    "            anno, self.tokenizer, self.max_seq_length\n",
    "        )\n",
    "        \n",
    "        input_ids, input_masks, segment_ids, label_ids, boxes = anno_objects\n",
    "        \n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "        input_masks = torch.tensor(input_masks, dtype=torch.long)\n",
    "        segment_ids = torch.tensor(segment_ids, dtype=torch.long)\n",
    "        label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "        boxes = torch.tensor(boxes, dtype=torch.long)\n",
    "        data = (\n",
    "            input_ids, input_masks, \n",
    "            segment_ids, label_ids, boxes,\n",
    "            img, mask\n",
    "        )\n",
    "        \n",
    "        return data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/results/combined/1606064001/'\n",
    "dataset = IDCardDataset(root=path, tokenizer=tokenizer)\n",
    "# len(data[0])\n",
    "input_ids, input_masks, segment_ids, label_ids, boxes, img, mask = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0],\n",
       "        [359, 755, 388, 828],\n",
       "        [377, 829, 395, 857],\n",
       "        ...,\n",
       "        [  0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0],\n",
       "        [  0,   0,   0,   0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "loader = DataLoader(dataset)\n",
    "input_ids, input_masks, segment_ids, label_ids, boxes, img, mask = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 9, 17, 30],\n",
       "          [ 9, 17, 30],\n",
       "          [ 9, 17, 30],\n",
       "          ...,\n",
       "          [ 0,  1, 11],\n",
       "          [ 0,  1, 11],\n",
       "          [ 0,  1, 11]],\n",
       "\n",
       "         [[ 9, 17, 30],\n",
       "          [ 9, 17, 30],\n",
       "          [ 9, 17, 30],\n",
       "          ...,\n",
       "          [ 0,  1, 11],\n",
       "          [ 0,  1, 11],\n",
       "          [ 0,  1, 11]],\n",
       "\n",
       "         [[ 9, 17, 30],\n",
       "          [ 9, 17, 30],\n",
       "          [ 9, 17, 30],\n",
       "          ...,\n",
       "          [ 0,  1, 11],\n",
       "          [ 0,  1, 11],\n",
       "          [ 0,  1, 11]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[20, 49, 80],\n",
       "          [21, 50, 81],\n",
       "          [24, 53, 84],\n",
       "          ...,\n",
       "          [24, 30, 43],\n",
       "          [24, 29, 44],\n",
       "          [24, 29, 44]],\n",
       "\n",
       "         [[18, 47, 78],\n",
       "          [20, 49, 80],\n",
       "          [22, 51, 82],\n",
       "          ...,\n",
       "          [24, 29, 44],\n",
       "          [25, 30, 45],\n",
       "          [25, 30, 45]],\n",
       "\n",
       "         [[17, 46, 77],\n",
       "          [18, 47, 78],\n",
       "          [20, 49, 80],\n",
       "          ...,\n",
       "          [24, 29, 44],\n",
       "          [24, 29, 44],\n",
       "          [25, 30, 45]]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('dlearn': conda)",
   "language": "python",
   "name": "python36864bitdlearnconda54b2ea4882264d059af3c47949da46dc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
